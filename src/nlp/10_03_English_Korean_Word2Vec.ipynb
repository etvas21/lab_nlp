{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.3 영어/한국어 Word2Vec 실습\n",
    "이번 챕터에서는 영어와 한국어 훈련 데이터에 대해서 Word2Vec을 학습해보겠습니다. gensim 패키지에서 Word2Vec은 이미 구현되어져 있으므로, 별도로 Word2Vec을 구현할 필요없이 손쉽게 훈련시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 영어 Word2Vec 만들기\n",
    "이번에는 영어 데이터를 다운로드 받아 직접 Word2Vec 작업을 진행해보겠습니다. 파이썬의 gensim 패키지에는 Word2Vec을 지원하고 있어, gensim 패키지를 이용하면 손쉽게 단어를 임베딩 벡터로 변환시킬 수 있습니다. 영어로 된 코퍼스를 다운받아 전처리를 수행하고, 전처리한 데이터를 바탕으로 Word2Vec 작업을 진행하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) 훈련 데이터 이해하기\n",
    "링크 : https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\n",
    "\n",
    "위 링크에서 훈련 데이터를 다운로드 받을 수 있습니다. 위 zip 파일의 압축을 풀면 ted_en-20160408.xml이라는 파일이 있는데, 이를 훈련 데이터로 사용할 예정입니다.\n",
    "\n",
    "xml 문법으로 작성되어져 있는 데이터이므로 자연어를 얻기 위해서는 전처리가 필요합니다. 얻고자 하는 실질적 데이터는 영어 문장으로만 구성된 <content>와 </content> 사이의 내용입니다. 해야할 일은 크게 두 가지네요. 첫째, 전처리 작업을 통해 xml 문법들은 제거합니다. 둘째, <content>와 </content> 사이 내용 중 (Laughter)나 (Applause)와 같은 배경음을 나타내는 단어들을 제거해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) 데이터 로드 및 전처리하기\n",
    "데이터를 다운로드하고, xml 파일로부터 필요한 내용만을 가져와 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from lxml import etree\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/ted_en-20160408.xml\", 'rb') as z:\n",
    "    target_text = etree.parse(z)\n",
    "    parse_text = '\\n'.join(target_text.xpath('//content/text()'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\\nTo me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation. Both are necessary, but it can be too much of a good thing.\\nConsider Facit\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
    "\n",
    "sent_text = sent_tokenize(content_text)\n",
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
    "\n",
    "normalized_text =[]\n",
    "for string in sent_text:\n",
    "    tokens = re.sub(r'[^a-z0-9]+',' ',string.lower())\n",
    "    normalized_text.append(tokens)\n",
    "\n",
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
    "\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count of total samples : 273424\n"
     ]
    }
   ],
   "source": [
    "print('\\nCount of total samples : {}'.format(len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "source": [
    "for line in result[:3]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상위 3개 문장만 출력해보았는데 토큰화가 수행되었음을 볼 수 있습니다. 이제 Word2Vec 모델에 텍스트 데이터를 훈련시킵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Word2Vec 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4,sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 Word2Vec의 하이퍼파라미터값은 다음과 같습니다.\n",
    "- size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\n",
    "- window = 컨텍스트 윈도우 크기\n",
    "- min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\n",
    "- workers = 학습을 위한 프로세스 수\n",
    "- sg = 0은 CBOW, 1은 Skip-gram.\n",
    "\n",
    "이제 Word2Vec에 대해서 학습을 진행하였습니다. Word2Vec는 입력한 단어에 대해서 가장 유사한 단어들을 출력하는 model.wv.most_similar을 지원합니다. man과 가장 유사한 단어들은 어떤 단어들일까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8475466966629028), ('guy', 0.8234093189239502), ('boy', 0.7667437791824341), ('girl', 0.7600693702697754), ('lady', 0.7567564249038696), ('soldier', 0.7446755170822144), ('kid', 0.711570143699646), ('gentleman', 0.7022470831871033), ('surgeon', 0.6656428575515747), ('poet', 0.6641961336135864)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model.wv.most_similar('man')\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "man과 유사한 단어로 woman, guy, boy, lady, girl, gentleman, soldier, kid 등을 출력하는 것을 볼 수 있습니다. 이제 Word2Vec를 통해 단어의 유사도를 계산할 수 있게 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Word2Vec 모델 저장하고 로드하기\n",
    "공들여 학습한 모델을 언제든 나중에 다시 사용할 수 있도록 컴퓨터 파일로 저장하고 다시 로드해보겠습니다. 이 모델을 가지고 시각화 챕터에서 시각화를 진행할 예정이므로 꼭 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model.wv.save_word2vec_format('eng_w2v')\n",
    "loaded_model = KeyedVectors.load_word2vec_format('eng_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8475466966629028), ('guy', 0.8234093189239502), ('boy', 0.7667437791824341), ('girl', 0.7600693702697754), ('lady', 0.7567564249038696), ('soldier', 0.7446755170822144), ('kid', 0.711570143699646), ('gentleman', 0.7022470831871033), ('surgeon', 0.6656428575515747), ('poet', 0.6641961336135864)]\n"
     ]
    }
   ],
   "source": [
    "model_result = loaded_model.most_similar('man')\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 한국어 Word2Vec 만들기(네이버 영화 리뷰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('10.3.ratings.txt', <http.client.HTTPMessage at 0x7fe6828935c0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt', filename='10.3.ratings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('10.3.ratings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           document  label\n",
       "0  8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1  8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2  4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "199992\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.dropna(how='any') # null값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any())\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규식을 이용하여 한글 외 문자 제거\n",
    "train_data['document'] = train_data['document'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고지금다시봐도재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을배우는학생으로외국디자이너와그들이일군전통을통해발전해가는문화산업이부러웠는데사실우...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리시리즈는부터뉴까지버릴께하나도없음최고</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와연기가진짜개쩔구나지루할거라고생각했는데몰입해서봤다그래이런게진짜영화지</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개자욱한밤하늘에떠있는초승달같은영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                  어릴때보고지금다시봐도재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을배우는학생으로외국디자이너와그들이일군전통을통해발전해가는문화산업이부러웠는데사실우...      1\n",
       "2   4655635                          폴리스스토리시리즈는부터뉴까지버릴께하나도없음최고      1\n",
       "3   9251303              와연기가진짜개쩔구나지루할거라고생각했는데몰입해서봤다그래이런게진짜영화지      1\n",
       "4  10067386                                안개자욱한밤하늘에떠있는초승달같은영화      1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 정의\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "okt = Okt()\n",
    "tokenized_data =[]\n",
    "\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = okt.morphs(sentence, stem=True)\n",
    "    temp_X = [ word for word in temp_X if not word in stopwords]\n",
    "    tokenized_data.append(temp_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of review : 68\n",
      "Average length of review :  10.669446777871116\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 리뷰 분포 확인\n",
    "print('Max length of review :', max(len(l) for l in tokenized_data))\n",
    "print('Average length of review : ', sum(map(len, tokenized_data))/len(tokenized_data))\n",
    "\n",
    "plt.hist([len(s) for s in tokenized_data], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences= tokenized_data , size=100,window=5, min_count=5,workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17947, 100)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 완성된 Embedding Matrix 크기 확인\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('서영희', 0.8768495321273804), ('한석규', 0.8588575124740601), ('윤제문', 0.8582131862640381), ('최민수', 0.8514682054519653), ('김명민', 0.8446720838546753), ('주진모', 0.8425426483154297), ('안성기', 0.8419971466064453), ('홉킨스', 0.8418986797332764), ('박해일', 0.8368625640869141), ('이주승', 0.8360577821731567)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('최민식'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('낭만', 0.7969143390655518), ('몽환', 0.7783679962158203), ('환상', 0.7559754848480225), ('향수', 0.7429937720298767), ('자극', 0.7366963624954224), ('복합', 0.7364035844802856), ('철학', 0.7336196899414062), ('비극', 0.7287598848342896), ('감각', 0.7253962159156799), ('직설', 0.7231409549713135)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('감성'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 한국어 Word2Vec 만들기(위키피디아)\n",
    "이번에는 위키피디아 한국어 덤프 파일을 다운받아서 한국어로 Word2Vec을 직접 진행해보겠습니다. 영어와 크게 다른 점은 없지만 한국어는 형태소 토큰화를 해야만 좋은 성능을 얻을 수 있습니다. 간단히 말해 형태소 분석기를 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) 위키피디아 한국어 덤프 파일 다운로드\n",
    "https://dumps.wikimedia.org/kowiki/latest/\n",
    "\n",
    "위 링크에는 많은 위키피디아 덤프 파일들이 존재합니다. 그 중에서 사용할 데이터는 kowiki-latest-pages-articles.xml.bz2 파일입니다. 해당 파일은 xml 파일므로, Word2Vec을 원활하게 진행하기 위해 파일 형식을 변환해줄 필요가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) 위키피디아 익스트랙터 다운로드\n",
    "해당 파일을 모두 다운로드 받았다면 위키피디아 덤프 파일을 텍스트 형식으로 변환시켜주는 오픈소스인 '위키피디아 익스트랙터'를 사용할 것입니다. '위키피디아 익스트랙터'를 다운로드 받기 위해서는 윈도우의 명령 프롬프트나 MAC과 리눅스의 터미널에서 아래의 git clone 명령어를 통해 다운로드 받을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "git clone \"https://github.com/attardi/wikiextractor.git\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) 위키피디아 한국어 덤프 파일 변환\n",
    "위키피디아 익스트랙터와 위키피디아 한국어 덤프 파일을 동일한 디렉토리 경로에 두고, 아래 명령어를 실행하면 위키피디아 덤프 파일이 텍스트 파일로 변환됩니다. 컴퓨터마다 다르지만 보통 10분 내외의 시간이 걸립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> python WikiExtractor.py kowiki-latest-pages-articles.xml.bz2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 파일로 변환된 위키피디아 한국어 덤프는 총 6개의 디렉토리(2018년 10월 기준)로 구성되어져 있습니다. AA ~ AF의 디렉토리로 각 디렉토리 내에는 wiki_00 ~ wiki_90이라는 파일들이 들어있습니다. 각 파일들을 열어보면 이와 같은 구성이 반복되고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<doc id=\"문서 번호\" url=\"실제 위키피디아 문서 주소\" title=\"문서 제목\">\n",
    "내용\n",
    "</doc>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어서 AA 디렉토리의 wiki_00 파일을 읽어보면, 지미 카터에 대한 내용이 나옵니다.\n",
    "\n",
    "이제 이 6개 AA ~ AF 디렉토리 안의 wiki00 ~ wiki90 파일들을 하나의 텍스트 파일로 통합하겠습니다. (만약, 더 간단히 하고 싶다면 모든 디렉토리 파일을 통합하지 않고, 하나의 디렉토리 내의 파일들에 대해서만 통합 작업을 진행하고 모델의 입력으로 사용할수도있습니다. 하지만 모델의 성능은 전체 파일에 대해서 진행한 경우보다 좋지 않을 수 있습니다.)\n",
    "\n",
    "작업은 6개의 디렉토리 내 파일들에 대해서 각 하나의 파일로 통합 후, 6개의 파일을 다시 하나로 통합하는 순서로 진행합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) 훈련 데이터 만들기\n",
    "우선 AA 디렉토리 안의 모든 파일인 wiki00 ~ wiki90에 대해서 wikiAA.txt로 통합해보겠습니다. 프롬프트에서 아래의 커맨드를 수행합니다. (윈도우 환경 기준)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell \n",
    "copy AA디렉토리의 경로\\wiki* wikiAA.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 커맨드는 AA디렉토리 안의 wiki로 시작되는 모든 파일을 wikiAA.txt 파일에 전부 복사하라는 의미를 담고있습니다. 결과적으로 wiki00 ~ wiki90파일의 모든 내용은 wikiAA.txt 파일이라는 하나의 파일에 내용이 들어가게 됩니다.\n",
    "\n",
    "각 디렉토리에 대해서도 동일하게 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "copy AB디렉토리의 경로\\wiki* wikiAB.txt\n",
    "copy AC디렉토리의 경로\\wiki* wikiAC.txt\n",
    "copy AD디렉토리의 경로\\wiki* wikiAD.txt\n",
    "copy AE디렉토리의 경로\\wiki* wikiAE.txt\n",
    "copy AF디렉토리의 경로\\wiki* wikiAF.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 되면 현재 경로에는 각 디렉토리의 파일들을 하나로 합친 wikiAA.txt 부터 wikiAF.txt라는 6개의 파일이 생깁니다. 그럼 이제 이 파일들에 대해서도 하나의 파일로 합치는 작업을 진행해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dos\n",
    "copy 현재 디렉토리의 경로\\wikiA* wiki_data.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) 훈련 데이터 전처리 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th line:\n",
      "<doc id=\"5\" url=\"https://ko.wikipedia.org/wiki?curid=5\" title=\"지미 카터\">\n",
      "\n",
      "2th line:\n",
      "지미 카터\n",
      "\n",
      "3th line:\n",
      "제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령 (1977년 ~ 1981년)이다.\n",
      "\n",
      "4th line:\n",
      "지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \"땅콩 농부\" (Peanut Farmer)로 알려졌다.\n",
      "\n",
      "5th line:\n",
      "1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/wiki_kor/wiki_data.txt', encoding='utf-8') as f:\n",
    "    i = 0 \n",
    "    while True:\n",
    "        line =f.readline()\n",
    "        if line != '\\n':\n",
    "            i +=1\n",
    "            print('%dth line:\\n'%i+line)\n",
    "        if i == 5:\n",
    "            break            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정상적으로 출력되는 것을 볼 수 있습니다. 이제 본격적으로 Word2Vec을 위한 학습 데이터를 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000Th While Sentence\n",
      "10000Th While Sentence\n",
      "15000Th While Sentence\n",
      "20000Th While Sentence\n",
      "25000Th While Sentence\n",
      "30000Th While Sentence\n",
      "35000Th While Sentence\n",
      "40000Th While Sentence\n",
      "45000Th While Sentence\n",
      "50000Th While Sentence\n",
      "55000Th While Sentence\n",
      "60000Th While Sentence\n",
      "65000Th While Sentence\n",
      "70000Th While Sentence\n",
      "75000Th While Sentence\n",
      "80000Th While Sentence\n",
      "85000Th While Sentence\n",
      "90000Th While Sentence\n",
      "95000Th While Sentence\n",
      "100000Th While Sentence\n",
      "105000Th While Sentence\n",
      "110000Th While Sentence\n",
      "115000Th While Sentence\n",
      "120000Th While Sentence\n",
      "125000Th While Sentence\n",
      "130000Th While Sentence\n",
      "135000Th While Sentence\n",
      "140000Th While Sentence\n",
      "145000Th While Sentence\n",
      "150000Th While Sentence\n",
      "155000Th While Sentence\n",
      "160000Th While Sentence\n",
      "165000Th While Sentence\n",
      "170000Th While Sentence\n",
      "175000Th While Sentence\n",
      "180000Th While Sentence\n",
      "185000Th While Sentence\n",
      "190000Th While Sentence\n",
      "195000Th While Sentence\n",
      "200000Th While Sentence\n",
      "205000Th While Sentence\n",
      "210000Th While Sentence\n",
      "215000Th While Sentence\n",
      "220000Th While Sentence\n",
      "225000Th While Sentence\n",
      "230000Th While Sentence\n",
      "235000Th While Sentence\n",
      "240000Th While Sentence\n",
      "245000Th While Sentence\n",
      "250000Th While Sentence\n",
      "255000Th While Sentence\n",
      "260000Th While Sentence\n",
      "265000Th While Sentence\n",
      "270000Th While Sentence\n",
      "275000Th While Sentence\n",
      "280000Th While Sentence\n",
      "285000Th While Sentence\n",
      "290000Th While Sentence\n",
      "295000Th While Sentence\n",
      "300000Th While Sentence\n",
      "305000Th While Sentence\n",
      "310000Th While Sentence\n",
      "315000Th While Sentence\n",
      "320000Th While Sentence\n",
      "325000Th While Sentence\n",
      "330000Th While Sentence\n",
      "335000Th While Sentence\n",
      "340000Th While Sentence\n",
      "345000Th While Sentence\n",
      "350000Th While Sentence\n",
      "355000Th While Sentence\n",
      "360000Th While Sentence\n",
      "365000Th While Sentence\n",
      "370000Th While Sentence\n",
      "375000Th While Sentence\n",
      "380000Th While Sentence\n",
      "385000Th While Sentence\n",
      "390000Th While Sentence\n",
      "395000Th While Sentence\n",
      "400000Th While Sentence\n",
      "405000Th While Sentence\n",
      "410000Th While Sentence\n",
      "415000Th While Sentence\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "fread = open('../../data/wiki_kor/wiki_data.txt', encoding='utf8')\n",
    "\n",
    "n = 0\n",
    "result =[]\n",
    "\n",
    "while True:\n",
    "    line = fread.readline()\n",
    "    if not line: break\n",
    "        \n",
    "    n += 1\n",
    "    \n",
    "    if n%5000 == 0:\n",
    "        print('%dTh While Sentence'%n)\n",
    "    \n",
    "    tokenlist = okt.pos(line, stem=True, norm=True)\n",
    "    temp =[]\n",
    "    \n",
    "    for word in tokenlist:\n",
    "        if word[1] in ['Noun']:\n",
    "            temp.append((word[0]))\n",
    "            \n",
    "    if temp:\n",
    "        result.append(temp)\n",
    "\n",
    "fread.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 형태소 분석기로 KoNLPy의 Okt를 사용하여 명사만을 추출하여 훈련 데이터를 구성하겠습니다. 위 작업은 시간이 꽤 걸립니다. 훈련 데이터를 모두 만들었다면, 훈련 데이터의 길이를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Count of total samples : {}'.format(len(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "약 240만여개의 줄(line)이 명사 토큰화가 되어 저장되어 있는 상태입니다. 이제 이를 Word2Vec으로 학습시킵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) Word2Vec 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(result, size=100, window=5, min_count=5, workers=4, sg=0)\n",
    "model.wv.save_word2vec_format('word2vec_wiki_train_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result1= movel.wv.most_similar('대한민국')\n",
    "print(model_result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result2 = model.wv.most_similar(\"어벤져스\")\n",
    "print(model_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result3 = model.wv.most_similar(\"반도체\")\n",
    "print(model_result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 사전 훈련된 Word2Vec 임베딩(Pre-trained Word2Vec embedding) 소개\n",
    "자연어 처리 작업을 할때, 케라스의 Embedding()를 사용하여 갖고 있는 훈련 데이터로부터 처음부터 임베딩 벡터를 훈련시키기도 하지만, 위키피디아 등의 방대한 데이터로 사전에 훈련된 워드 임베딩(pre-trained word embedding vector)를 가지고 와서 해당 벡터들의 값을 원하는 작업에 사용 할 수도 있습니다.\n",
    "\n",
    "예를 들어서 감성 분류 작업을 하는데 훈련 데이터의 양이 부족한 상황이라면, 다른 방대한 데이터를 Word2Vec이나 GloVe 등으로 사전에 학습시켜놓은 임베딩 벡터들을 가지고 와서 모델의 입력으로 사용하는 것이 때로는 더 좋은 성능을 얻을 수 있습니다.\n",
    "\n",
    "여기서는 사전 훈련된 워드 임베딩을 가져와서 간단히 단어들의 유사도를 구해보는 실습을 해보겠습니다. 실제로 모델에 적용해보는 실습은 사전 훈련된 워드 임베딩 챕터에서 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) 영어\n",
    "이번에는 구글이 제공하는 사전 훈련된(미리 학습되어져 있는) Word2Vec 모델을 사용하는 방법에 대해서 알아보도록 하겠습니다. 구글은 사전 훈련된 3백만 개의 Word2Vec 단어 벡터들을 제공합니다. 각 임베딩 벡터의 차원은 300입니다. gensim을 통해서 이 모델을 불러오는 건 매우 간단합니다. 이 모델을 다운로드하고 파일 경로를 기재하면 됩니다.\n",
    "\n",
    "모델 다운로드 경로 : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "압축 파일의 용량은 약 1.5GB이지만, 파일의 압축을 풀면 약 3.3GB의 파일이 나옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/word2vec/GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-66dbdcf8aaed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 구글의 사전 훈련된 Word2Vec 모델을 로드합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/word2vec/GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/word2vec/GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# 구글의 사전 훈련된 Word2Vec 모델을 로드합니다.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../../data/word2vec/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'vectors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-bf2688598082>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'vectors'"
     ]
    }
   ],
   "source": [
    "print(model.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 크기는 3,000,000 x 300. 즉, 3백만 개의 단어와 각 단어의 차원은 300\n",
    "파일의 크기가 3기가가 넘는 이유를 계산해보면 아래와 같다.\n",
    "3 million words * 300 features * 4bytes/feature = ~3.35GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06644509\n",
      "0.2374398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('this', 'is'))\n",
    "print(model.similarity('post', 'book'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.18859726  0.22162203  0.20745376 -1.4965757  -0.69844955 -0.00980035\n",
      "  3.4987075   0.384841    1.1622745   2.628655    0.43163773  0.7573469\n",
      " -1.306633    0.35188112  0.7643188  -0.19475071  0.50412315 -1.1889123\n",
      "  1.3502835  -1.2910469   1.2803894  -0.713317    0.2740907   0.6238029\n",
      "  0.63854206 -0.6661981  -2.5077026   0.34835234 -0.27336     0.48202297\n",
      "  0.4533118  -1.7291496   2.0528414   0.7557329  -1.2700756   1.0946919\n",
      "  1.0056558   0.42851084  1.1962385   1.0706497   1.5378739   0.7705777\n",
      "  0.56205136 -1.320664    1.2173762   0.19921641 -1.3601569  -0.18224223\n",
      "  1.1140269  -1.3078339  -1.7141244  -0.27227315  1.7753294   0.70721227\n",
      " -0.515068    0.86843914 -0.42534164 -1.5325965  -0.99583906  0.6315444\n",
      "  0.0770057   1.5918869  -1.247401   -0.1988752   0.03625113  1.2565724\n",
      " -0.87707216 -0.57662153  0.7291584   0.6037869  -0.36633137  0.49734905\n",
      " -1.0564293   0.67495775 -0.28606537  1.0797621  -0.8946842   0.03722539\n",
      " -0.6903274   1.7497684   2.063583   -0.68122387  0.07788887 -0.4107822\n",
      "  0.921901    0.44431224  0.05165813  1.1006261  -0.03985     1.1238618\n",
      "  0.70432407 -0.22508517  0.24404539  2.6557162  -0.4080935   1.5059702\n",
      "  0.12619834  0.29597205 -2.1643274  -0.2069252 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model['book']) # 단어 book의 백터 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) 한국어\n",
    "한국어의 미리 학습된 Word2Vec 모델은 박규병님의 깃허브 주소인 https://github.com/Kyubyong/wordvectors 에 공개되어져 있습니다. 박규병님이 공개한 직접적인 다운로드 링크는 아래와 같습니다.\n",
    "\n",
    "모델 다운로드 경로 : https://drive.google.com/file/d/0B0ZXk88koS2KbDhXdWg1Q2RydlU/view\n",
    "\n",
    "위의 링크로부터 77MB 크기의 ko.zip 파일을 다운로드 받아서 압축을 풀면 ko.bin이라는 50MB 크기의 파일이 있습니다. 이 파일을 로드하고 유사도를 계산해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '../../data/word2vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-09794bede73d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/word2vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \"\"\"\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0;31m# for backward compatibility for `max_final_vocab` feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \"\"\"\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m     \"\"\"\n\u001b[0;32m-> 1381\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '../../data/word2vec'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec.load('../../data/word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word '강아지' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-28f741b00bb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'강아지'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '강아지' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "result = model.wv.most_similar('강아지')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 : Word2vec 모델은 자연어 처리에서 단어를 밀집 벡터로 만들어주는 단어 임베딩 방법론이지만 최근에 들어서는 자연어 처리를 넘어서 추천 시스템에도 사용되고 있는 모델입니다. 우선 적당하게 데이터를 나열해주면 Word2vec은 위치가 근접한 데이터를 유사도가 높은 벡터를 만들어준다는 점에서 착안된 아이디어입니다.\n",
    "\n",
    "링크 : https://brunch.co.kr/@goodvc78/16?fbclid=IwAR1QZZAeZe_tNWxnxVCRwl8PIouBPAaqSIJ1lBxJ-EKtfDfmLehi1MUV_Lk\n",
    "\n",
    "위 링크에서 그림 10번을 보면 쉽게 이해할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
