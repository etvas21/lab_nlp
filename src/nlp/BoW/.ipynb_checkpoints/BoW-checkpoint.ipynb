{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words(BoW)\n",
    "단어의 등장 순서를 고려하지 않는 빈도수 기반의 단어 표현 방법인 Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag of Words란?\n",
    "Bag of Words란 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법입니다. \n",
    "\n",
    "Bag of Words를 직역하면 단어들의 가방이라는 의미입니다. 단어들이 들어있는 가방을 상상해봅시다. 갖고있는 어떤 텍스트 문서에 있는 단어들을 가방에다가 전부 넣습니다. 그러고나서 이 가방을 흔들어 단어들을 섞습니다. 만약, 해당 문서 내에서 특정 단어가 N번 등장했다면, 이 가방에는 그 특정 단어가 N개 있게됩니다. 또한 가방을 흔들어서 단어를 섞었기 때문에 더 이상 단어의 순서는 중요하지 않습니다.\n",
    "\n",
    "BoW를 만드는 과정을 이렇게 두 가지 과정으로 생각해보겠습니다.\n",
    "1. 우선, 각 단어에 고유한 정수 인덱스를 부여합니다.\n",
    "2. 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt   #. Okt(Open Korea Text)는 원래 이름이 Twitter였으나 0.5.0 버전 이후부터 이름이 Okt로 바뀌었다.\n",
    "import re\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "sentense = \"한글 한글은 한글로. LG CNS가 국내 처음이자 유일하게 AI의 연어 이해를 위한 AI 학습용 표준데이터 ‘코쿼드 2.0(KorQuAD 2.0)’를 5일 공개하고 국내 AI 업계에 무료로 개방한다고 발표했다.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한글', '한글', '은', '한글', '로', '.', 'LG', 'CNS', '가', '국내', '처음', '이자', '유일하게', 'AI', '의', '연어', '이해', '를', '위', '한', 'AI', '학습', '용', '표준', '데이터', '‘', '코쿼드', '2.0', '(', 'KorQuAD', '2.0', ')’', '를', '5일', '공개', '하고', '국내', 'AI', '업계', '에', '무료', '로', '개방', '한', '다', '고', '발표', '했다', '.']\n",
      "\n",
      "\n",
      " ['한글', '한글', '은', '한글', '로', '.', 'LG', 'CNS', '가다', '국내', '처음', '이자', '유일하다', 'AI', '의', '연어', '이해', '를', '위', '한', 'AI', '학습', '용', '표준', '데이터', '‘', '코쿼드', '2.0', '(', 'KorQuAD', '2.0', ')’', '를', '5일', '공개', '하고', '국내', 'AI', '업계', '에', '무료', '로', '개방', '한', '다', '고', '발표', '하다', '.']\n",
      "\n",
      "\n",
      " ['한글', '한글', '한글', '국내', '처음', '이자', '의', '연어', '이해', '위', '학습', '용', '표준', '데이터', '코쿼드', '를', '공개', '국내', '업계', '무료', '개방', '고', '발표']\n",
      "\n",
      "\n",
      " [('한글', 'Noun'), ('한글', 'Noun'), ('은', 'Josa'), ('한글', 'Noun'), ('로', 'Josa'), ('.', 'Punctuation'), ('LG', 'Alpha'), ('CNS', 'Alpha'), ('가', 'Verb'), ('국내', 'Noun'), ('처음', 'Noun'), ('이자', 'Noun'), ('유일하게', 'Adjective'), ('AI', 'Alpha'), ('의', 'Noun'), ('연어', 'Noun'), ('이해', 'Noun'), ('를', 'Josa'), ('위', 'Noun'), ('한', 'Josa'), ('AI', 'Alpha'), ('학습', 'Noun'), ('용', 'Noun'), ('표준', 'Noun'), ('데이터', 'Noun'), ('‘', 'Foreign'), ('코쿼드', 'Noun'), ('2.0', 'Number'), ('(', 'Punctuation'), ('KorQuAD', 'Alpha'), ('2.0', 'Number'), (')’', 'Punctuation'), ('를', 'Noun'), ('5일', 'Number'), ('공개', 'Noun'), ('하고', 'Josa'), ('국내', 'Noun'), ('AI', 'Alpha'), ('업계', 'Noun'), ('에', 'Josa'), ('무료', 'Noun'), ('로', 'Josa'), ('개방', 'Noun'), ('한', 'Josa'), ('다', 'Adverb'), ('고', 'Noun'), ('발표', 'Noun'), ('했다', 'Verb'), ('.', 'Punctuation')]\n",
      "Noun\n"
     ]
    }
   ],
   "source": [
    "print(okt.morphs(sentense))                      # 형태소분석\n",
    "print('\\n\\n',okt.morphs(sentense, stem=True))   # 어간추출\n",
    "print('\\n\\n',okt.nouns(sentense))   # 명사\n",
    "#print('\\n\\n',okt.phrases(sentense)) # 어절\n",
    "print('\\n\\n',okt.pos(sentense))     # 품사태깅\n",
    "xpos = okt.pos(sentense)\n",
    "print(xpos[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(sentense):\n",
    "    vocab = okt.morphs(sentense)\n",
    "    print(\"All word class\\n\", vocab)\n",
    "    \n",
    "    #'Alpha','Noun','Number', 'Verb', 'Adjective', 'Adverb', 'Punctuation'\n",
    "    word_class = ['Alpha','Noun','Number', 'Adjective']\n",
    "    xvocab = okt.pos(sentense)\n",
    "    vocab =[]\n",
    "    \n",
    "    for word, wc in xvocab:\n",
    "        if wc in word_class and len(word)>1:\n",
    "            vocab.append(word)\n",
    "    \n",
    "    print(\"\\nword_class = ['Alpha','Noun','Number', 'Adjective']\\n\",vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(vocab):\n",
    "    ''' cleansing '''\n",
    "    xvocab = []\n",
    "    for word in vocab:\n",
    "        x  = re.sub(\"[^0-9a-zA-Z가-힣]\", \"\",word)\n",
    "        if len(x) > 0:\n",
    "            xvocab.append(re.sub(\"[^0-9a-zA-Z가-힣]\", \"\",word))\n",
    "\n",
    "    print(\"[^0-9a-zA-Z가-힣]\\n\",xvocab)\n",
    "    return xvocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow(vocab):   \n",
    "    idx_vocab = {}\n",
    "    bow =[]\n",
    "\n",
    "    for word in vocab:\n",
    "        if word not in idx_vocab.keys():\n",
    "            idx_vocab[word] = len(idx_vocab)\n",
    "            bow.insert(len(idx_vocab)-1,1)\n",
    "        else:\n",
    "            idx = idx_vocab.get(word)\n",
    "            bow[idx] = bow[idx]+1\n",
    "\n",
    "    print(idx_vocab,'\\n', bow, '\\n\\n')\n",
    "\n",
    "    xdf = pd.DataFrame(list(idx_vocab.keys()), columns=['word'])\n",
    "    xdf['cnt'] = bow\n",
    "    xdf.sort_values(by=['cnt'],axis=0,ascending=False, inplace=True)\n",
    "    \n",
    "    return xdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All word class\n",
      " ['한글', '한글', '은', '한글', '로', '.', 'LG', 'CNS', '가', '국내', '처음', '이자', '유일하게', 'AI', '의', '연어', '이해', '를', '위', '한', 'AI', '학습', '용', '표준', '데이터', '‘', '코쿼드', '2.0', '(', 'KorQuAD', '2.0', ')’', '를', '5일', '공개', '하고', '국내', 'AI', '업계', '에', '무료', '로', '개방', '한', '다', '고', '발표', '했다', '.']\n",
      "\n",
      "word_class = ['Alpha','Noun','Number', 'Adjective']\n",
      " ['한글', '한글', '한글', 'LG', 'CNS', '국내', '처음', '이자', '유일하게', 'AI', '연어', '이해', 'AI', '학습', '표준', '데이터', '코쿼드', '2.0', 'KorQuAD', '2.0', '5일', '공개', '국내', 'AI', '업계', '무료', '개방', '발표']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "vocab = word_tokenize(sentense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[^0-9a-zA-Z가-힣]\n",
      " ['한글', '한글', '한글', 'LG', 'CNS', '국내', '처음', '이자', '유일하게', 'AI', '연어', '이해', 'AI', '학습', '표준', '데이터', '코쿼드', '20', 'KorQuAD', '20', '5일', '공개', '국내', 'AI', '업계', '무료', '개방', '발표']\n"
     ]
    }
   ],
   "source": [
    "# Cleansing\n",
    "vocab = cleansing(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'한글': 0, 'LG': 1, 'CNS': 2, '국내': 3, '처음': 4, '이자': 5, '유일하게': 6, 'AI': 7, '연어': 8, '이해': 9, '학습': 10, '표준': 11, '데이터': 12, '코쿼드': 13, '20': 14, 'KorQuAD': 15, '5일': 16, '공개': 17, '업계': 18, '무료': 19, '개방': 20, '발표': 21} \n",
      " [3, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>한글</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AI</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>국내</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>데이터</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>개방</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>무료</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>업계</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>공개</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5일</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>KorQuAD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>코쿼드</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>표준</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>학습</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>이해</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>연어</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>유일하게</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>이자</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>처음</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>발표</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  cnt\n",
       "0        한글    3\n",
       "7        AI    3\n",
       "3        국내    2\n",
       "14       20    2\n",
       "12      데이터    1\n",
       "20       개방    1\n",
       "19       무료    1\n",
       "18       업계    1\n",
       "17       공개    1\n",
       "16       5일    1\n",
       "15  KorQuAD    1\n",
       "13      코쿼드    1\n",
       "11       표준    1\n",
       "1        LG    1\n",
       "10       학습    1\n",
       "9        이해    1\n",
       "8        연어    1\n",
       "6      유일하게    1\n",
       "5        이자    1\n",
       "4        처음    1\n",
       "2       CNS    1\n",
       "21       발표    1"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make BoW\n",
    "make_bow(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW는 각 단어가 등장한 횟수를 수치화하는 텍스트 표현 방법이기 때문에, 주로 어떤 단어가 얼마나 등장했는지를 기준으로 문서가 어떤 성격의 문서인지를 판단하는 작업에 사용. \n",
    "\n",
    "즉, 분류 문제나 여러 문서 간의 유사도를 구하는 문제에 주로 쓰입니다. \n",
    "- 가령, '달리기', '체력', '근력'과 같은 단어가 자주 등장하면 해당 문서를 체육 관련 문서로 분류할 수 있을 것이며,\n",
    "- '미분', '방정식', '부등식'과 같은 단어가 자주 등장한다면 수학 관련 문서로 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. CountVectorizer 클래스로 BoW 만들기\n",
    "사이킷 런에서는 단어의 빈도를 Count하여 Vector로 만드는 CountVectorizer 클래스를 지원합니다. 이를 이용하면 영어에 대해서는 손쉽게 BoW를 만들 수 있습니다. CountVectorizer로 간단하고 빠르게 BoW를 만드는 실습을 진행해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency => [[1 1 2 1 2 1 1 1 1]]\n",
      "Word/Index => {'you': 4, 'know': 1, '한글': 6, '한글은': 8, '한글로': 7, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>idx</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>because</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>know</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>want</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>your</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>한글</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>한글로</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>한글은</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  idx  cnt\n",
       "2     love    2    2\n",
       "4      you    4    2\n",
       "0  because    0    1\n",
       "1     know    1    1\n",
       "3     want    3    1\n",
       "5     your    5    1\n",
       "6       한글    6    1\n",
       "7      한글로    7    1\n",
       "8      한글은    8    1"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['you know 한글 한글은 한글로 I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# Record the frequency of each word from the Corpus\n",
    "frq = vector.fit_transform(corpus).toarray()\n",
    "print('frequency =>', frq)\n",
    "print('Word/Index =>', vector.vocabulary_)\n",
    "\n",
    "# Show how each word is indexed.\n",
    "cv_vocab = sorted(vector.vocabulary_.items(), key=lambda x: x[1])\n",
    "cvdf = pd.DataFrame(list(cv_vocab),columns=['word', 'idx'])\n",
    "cvdf['cnt'] = frq[0]\n",
    "cvdf.sort_values(by=['cnt'],axis=0,ascending=False, inplace=True)\n",
    "cvdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제 문장에서 \n",
    "\n",
    "- you와 love는 두 번씩 언급되었으므로 각각 인덱스 2와 인덱스 4에서 2의 값을 가지며, 그 외의 값에서는 1의 값을 가지는 것을 볼 수 있습니다. \n",
    "- 또한 알파벳 I는 BoW를 만드는 과정에서 사라졌는데, 이는 CountVectorizer가 기본적으로 길이가 2이상인 문자에 대해서만 토큰으로 인식하기 때문입니다. \n",
    "- 정제(Cleaning) 챕터에서 언급했듯이, 영어에서는 길이가 짧은 문자를 제거하는 것 또한 전처리 작업으로 고려되기도 합니다.\n",
    "\n",
    "주의할 것은 CountVectorizer는 단지 띄어쓰기만을 기준으로 단어를 자르는 낮은 수준의 토큰화를 진행하고 BoW를 만든다는 점입니다. \n",
    "\n",
    "- 이는 영어의 경우 띄어쓰기만으로 토큰화가 수행되기 때문에 문제가 없지만 \n",
    "- 한국어에 CountVectorizer를 적용하면, 조사 등의 이유로 제대로 BoW가 만들어지지 않음을 의미합니다.\n",
    "\n",
    "위의 예에서 '...한글 한글은 한글로...' 의 단어는 \n",
    "\n",
    "- CountVectorizer는 '한글'이라는 단어를 인식하지 못 합니다. \n",
    "- CountVectorizer는 띄어쓰기를 기준으로 분리한 뒤에 '한글은'와 '한글로'는 조사를 포함해서 하나의 단어로 판단하기 때문에 서로 다른 두 단어로 인식합니다. \n",
    "- 그래서 '한글은'과 '한글로'는 각자 다른 인덱스에서 1이라는 빈도의 값을 갖게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 불용어를 제거한 BoW 만들기\n",
    "앞서 불용어는 자연어 처리에서 별로 의미를 갖지 않는 단어들이라고 언급한 바 있습니다. \n",
    "- BoW를 사용한다는 것은 그 문서에서 각 단어가 얼마나 자주 등장했는지를 보겠다는 것입니다. \n",
    "- 그리고 각 단어에 대한 빈도수를 수치화 하겠다는 것은 결국 텍스트 내에서 어떤 단어들이 중요한지를 보고싶다는 의미를 함축하고 있습니다. \n",
    "- 그렇다면 BoW를 만들때 불용어를 제거하는 일은 자연어 처리의 정확도를 높이기 위해서 선택할 수 있는 전처리 기법입니다.\n",
    "\n",
    "영어의 BoW를 만들기 위해 사용하는 CountVectorizer는 불용어를 지정하면, 불용어는 제외하고 BoW를 만들 수 있도록 불용어 제거 기능을 지원하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) 사용자가 직접 정의한 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
    "\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) CountVectorizer에서 제공하는 자체 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]]\n",
      "{'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) NLTK에서 지원하는 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "\n",
    "sw = stopwords.words(\"english\")\n",
    "vect = CountVectorizer(stop_words=sw)\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
