{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14.2 텍스트 요약(Text Summarization with Attention mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 요약은 상대적으로 큰 원문을 핵심 내용만 간추려서 상대적으로 작은 요약문으로 변환하는 것을 말합니다. 읽는 사람이 시간을 단축해서 내용을 빠르게 이해할 수 있다는 점에서 글을 많이 쓰는 사람들에게는 꼭 필요한 능력 중 하나인 것 같습니다. 그런데 만약 기계가 이를 자동으로 해줄 수만 있다면 얼마나 좋을까요? 이번 챕터에서는 그 중 한 가지 방법인 seq2seq를 구현해보겠습니다. 그리고 어텐션 메커니즘(attention mechanism)을 적용해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 텍스트 요약(Text Summarization)\n",
    "텍스트 요약은 크게 추출적 요약(extractive summarization)과 추상적 요약(abstractive summarization)으로 나뉩니다.\n",
    "\n",
    "##### 1) 추출적 요약(extractive summarization)\n",
    "추출적 요약은 원문에서 중요한 핵심 문장 또는 단어구를 몇 개 뽑아서 이들로 구성된 요약문을 만드는 방법입니다. 그렇기 때문에 추출적 요약의 결과로 나온 요약문의 문장이나 단어구들은 전부 원문에 있는 문장들입니다. 추출적 요약의 대표적인 알고리즘으로 머신 러닝 알고리즘인 텍스트랭크(TextRank)가 있는데, 아래의 링크에서 텍스트랭크로 구현된 세 줄 요약기를 시험해볼 수 있습니다.\n",
    "\n",
    "링크 : https://summariz3.herokuapp.com/\n",
    "\n",
    "위 링크로 이동하여 인터넷 뉴스나 가지고 있는 글을을 복사 + 붙여넣기하여 결과를 살펴보세요. 세 개의 문장은 전부 원문에 존재하던 문장들입니다. 이 방법의 단점이라면, 이미 존재하는 문장이나 단어구로만 구성하므로 모델의 언어 표현 능력이 제한된다는 점입니다.\n",
    "\n",
    "그렇다면 마치 사람처럼 원문에 없던 단어나 문장을 사용하면서 핵심만 간추려서 표현하는 요약 방법은 없을까요?\n",
    "\n",
    "##### 2) 추상적 요약(abstractive summarization)\n",
    "추상적 요약은 원문에 없던 문장이라도 핵심 문맥을 반영한 새로운 문장을 생성해서 원문을 요약하는 방법입니다. 마치 사람이 요약하는 것 같은 방식인데, 당연히 추출적 요약보다는 난이도가 높습니다. 이 방법은 주로 인공 신경망을 사용하며 대표적인 모델로 seq2seq가 있습니다. 이 방법의 단점이라면 seq2seq와 같은 인공 신경망들은 기본적으로 지도 학습이라는 점입니다. 다시 말해 추상적 요약을 인공 신경망으로 훈련하기 위해서는 '원문' 뿐만 아니라 '실제 요약문'이라는 레이블 데이터가 있어야 합니다. 그렇기 때문에 데이터를 구성하는 것 자체가 하나의 부담입니다. 이번 챕터에서는 이미 공개된 데이터를 사용해서 추상적 요약을 실습해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 아마존 리뷰 데이터에 대한 이해\n",
    "이번 챕터에서 사용할 데이터는 아마존 리뷰 데이터입니다. 아래의 링크에서 데이터를 다운로드합니다.\n",
    "\n",
    "링크 : https://www.kaggle.com/snap/amazon-fine-food-reviews\n",
    "\n",
    "우선 실습에 필요한 도구들을 임포트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) 데이터 로드하기\n",
    "Reviews.csv 파일을 불러와 데이터프레임에 저장하겠습니다. 이 데이터는 실제로는 약 56만개의 샘플을 가지고 있습니다. 하지만 여기서는 간단히 10만개의 샘플만 사용하겠습니다. 이는 pd.read_csv의 nrows의 인자로 10만이라는 숫자를 적어주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review count :  100000\n"
     ]
    }
   ],
   "source": [
    "df_data = pd.read_csv('../../data/kaggle/amazon-fine-food-reviews.csv', nrows=100000)\n",
    "print('Review count : ', len(df_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5개의 샘플을 출력해보면 'Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'이라는 10개의 열이 존재함을 알 수 있습니다. 그런데 사실 이 중 필요한 열은 'Text'열과 'Summary'열 뿐입니다.\n",
    "\n",
    "Text열과 Summary열만을 분리하고, 다른 열들은 데이터에서 제외시켜서 재저장합니다. 그리고 5개의 샘플을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>Not as Advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>Cough Medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>Great taffy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary\n",
       "0  I have bought several of the Vitality canned d...  Good Quality Dog Food\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...      Not as Advertised\n",
       "2  This is a confection that has been around a fe...  \"Delight\" says it all\n",
       "3  If you are looking for the secret ingredient i...         Cough Medicine\n",
       "4  Great taffy at a great price.  There was a wid...            Great taffy"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = df_data[['Text', 'Summary']]\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text열과 Summary열만 저장된 것을 확인할 수 있습니다. Text열이 원문이고, Summary열이 Text열에 대한 요약입니다. 다시 말해 모델은 Text(원문)으로부터 Summary(요약)을 예측하도록 훈련됩니다. 랜덤으로 샘플 몇 가지를 더 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>OMG I would love these cookies except for the ...</td>\n",
       "      <td>OMG DO NOT BUY!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65382</th>\n",
       "      <td>Celestial Seasonings Green Tea, Antioxidant ty...</td>\n",
       "      <td>Gracious Green Goodness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15141</th>\n",
       "      <td>This is the very best maple syrup you can buy ...</td>\n",
       "      <td>Less water = optimal maple flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98122</th>\n",
       "      <td>My 2 year old loves the squeeze applesauce so ...</td>\n",
       "      <td>My toddler would not eat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87501</th>\n",
       "      <td>I tried a few of these then threw the rest awa...</td>\n",
       "      <td>More medicinal taste than vanilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15344</th>\n",
       "      <td>We had been looking for a healthy waffle mix t...</td>\n",
       "      <td>Perfect Waffle mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3936</th>\n",
       "      <td>Best coffee ever. Still haven't found anything...</td>\n",
       "      <td>Smooth Brown Wonderfulness!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47459</th>\n",
       "      <td>These are SOOOO good. They are as good if not ...</td>\n",
       "      <td>AWESOME!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86701</th>\n",
       "      <td>Whether you like the basic fruit smoothie or a...</td>\n",
       "      <td>gourmet item at its best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13936</th>\n",
       "      <td>Flamin' Buffalo Chick + A1 are my all time fav...</td>\n",
       "      <td>Love it!  But huge on sodium though.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "967    OMG I would love these cookies except for the ...   \n",
       "65382  Celestial Seasonings Green Tea, Antioxidant ty...   \n",
       "15141  This is the very best maple syrup you can buy ...   \n",
       "98122  My 2 year old loves the squeeze applesauce so ...   \n",
       "87501  I tried a few of these then threw the rest awa...   \n",
       "15344  We had been looking for a healthy waffle mix t...   \n",
       "3936   Best coffee ever. Still haven't found anything...   \n",
       "47459  These are SOOOO good. They are as good if not ...   \n",
       "86701  Whether you like the basic fruit smoothie or a...   \n",
       "13936  Flamin' Buffalo Chick + A1 are my all time fav...   \n",
       "\n",
       "                                    Summary  \n",
       "967                       OMG DO NOT BUY!!!  \n",
       "65382               Gracious Green Goodness  \n",
       "15141     Less water = optimal maple flavor  \n",
       "98122              My toddler would not eat  \n",
       "87501     More medicinal taste than vanilla  \n",
       "15344                    Perfect Waffle mix  \n",
       "3936            Smooth Brown Wonderfulness!  \n",
       "47459                              AWESOME!  \n",
       "86701              gourmet item at its best  \n",
       "13936  Love it!  But huge on sodium though.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 data.sample(10)를 한 번만 실행했지만 지속적으로 몇 차례 더 실행하면서 샘플의 구조를 확인해보세요. 원문은 꽤 긴 반면에, Summary에는 3~4개의 단어만으로 구성된 경우도 많아보입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) 데이터 정제하기\n",
    "데이터에 중복 샘플이 있는지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 2 columns):\n",
      "Text       100000 non-null object\n",
      "Summary    99998 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 열에서 중복을 배제한 유일한 샘플의 수 : 88426\n",
      "Summary 열에서 중복을 배제한 유일한 샘플의 수 : 72348\n"
     ]
    }
   ],
   "source": [
    "print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', df_data['Text'].nunique())\n",
    "print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', df_data['Summary'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 데이터는 10만개의 샘플이 존재하지만, 실제로는 꽤 많은 원문이 중복되어 중복을 배제한 유일한 원문의 개수는 88,426개입니다. 중복 샘플이 무려 약 1,200개나 있다는 의미지요. Summary는 중복이 더 많지만, 원문은 다르더라도 짧은 문장인 요약은 내용이 겹칠 수 있음을 가정하고 일단 두겠습니다. Summary의 길이 분포는 뒤에서 확인하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.drop_duplicates(subset=['Text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 88426 entries, 0 to 99999\n",
      "Data columns (total 2 columns):\n",
      "Text       88426 non-null object\n",
      "Summary    88425 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary에서 1개의 Null data 가 존재함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 88425 entries, 0 to 99999\n",
      "Data columns (total 2 columns):\n",
      "Text       88425 non-null object\n",
      "Summary    88425 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_data.dropna(axis=0, inplace=True)\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 남은 샘플 수는 88,425개입니다. 지금까지는 불필요한 샘플의 수를 줄이기 위한 정제 과정이었습니다. 이제 샘플 내부를 전처리해야 합니다. 단어 정규화와 불용어 제거를 위해 각각의 참고 자료가 필요합니다. 동일한 의미를 가졌지만 스펠링이 다른 단어들을 정규화하기 위한 사전을 만듭니다. 이 사전은 아래의 링크를 참고하여 만들어진 사전입니다.\n",
    "\n",
    "링크 : https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수 내 사용\n",
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK의 불용어를 저장하고 개수를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Stop word :  179\n",
      "{'our', 'were', 'so', 'for', 'against', \"shouldn't\", 'them', 'its', 'at', 'theirs', 'than', 'hadn', \"she's\", 'been', 'both', 'about', 'you', \"you'll\", 'very', \"mightn't\", 'before', 'needn', 'mightn', 'ourselves', 'while', 'she', 'being', 'why', \"didn't\", 'hasn', 'wasn', 'himself', 'most', 'won', 'weren', 'no', \"that'll\", \"hasn't\", 'such', 'did', 'should', 'there', 'haven', 'these', 'o', 'between', \"it's\", 'by', 'only', 'wouldn', 'from', 'didn', 'doesn', 'where', 'i', 'the', 'mustn', 'that', 'few', 'which', 'all', \"mustn't\", \"won't\", 'me', 'my', 're', 'm', 'he', 'it', 'themselves', \"wouldn't\", 'we', 'above', 'have', 'now', \"don't\", 'be', 'too', \"should've\", 'can', 'yours', 'again', 'each', 'isn', 'y', 'him', 'after', 'having', 'is', 'because', 'd', 'herself', 'ma', 'up', 'nor', \"isn't\", \"you've\", 'then', 'same', \"haven't\", 'any', 'when', 'own', 'are', \"couldn't\", 'and', 'this', 'through', 'doing', 'on', 'does', 'here', 'am', 'had', 'couldn', 'shan', 'or', 'will', 'with', 'itself', 'in', 'once', 'below', 'was', 'll', \"needn't\", 'his', 'her', 'during', 'just', 'as', 'off', 'myself', \"weren't\", 'out', \"you're\", 'some', \"hadn't\", 'do', 've', 'who', 'ours', 'don', \"wasn't\", 'whom', 'over', 'not', 'under', 'an', 'other', \"you'd\", 'how', 'until', 'down', 'what', 'hers', 'more', 's', 'shouldn', 'but', 'into', \"doesn't\", 'of', 'their', 'ain', \"shan't\", 't', 'has', 'aren', 'if', 'yourself', 'to', 'they', \"aren't\", 'those', 'a', 'further', 'yourselves', 'your'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print('Count of Stop word : ', len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 함수를 설계합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    # to lower case\n",
    "    sentence = sentence.lower()\n",
    "    #print('\\nTo lower case\\n', sentence)\n",
    "    \n",
    "    # Remove html tags\n",
    "    sentence = BeautifulSoup(sentence, 'lxml').text\n",
    "    #print('\\nRemove html tags\\n', sentence)\n",
    "    \n",
    "    # 괄호로 닫힌 문자열 제거 Ex) my husband (and myself) for => my husband for\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', str(sentence))\n",
    "    #print('\\nRemove with parentheses\\n', sentence)\n",
    "    \n",
    "    # 쌍따옴표 제거\n",
    "    sentence = re.sub('\"', '', sentence)\n",
    "    #print('\\nRemove double quotation\\n', sentence)\n",
    "    \n",
    "    # 약어 정규화\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")])\n",
    "    #print('\\nNormalization of abbreviation\\n', sentence)\n",
    "    \n",
    "    # 소유격 제거   Ex) roland's -> roland\n",
    "    sentence = re.sub(r\"'s\\b\", '', sentence)\n",
    "    #print('\\nRemove possessive\\n', sentence)\n",
    "    \n",
    "    # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    #print('\\nConvert non-English  character  to spaces\\n', sentence)\n",
    "    \n",
    "    # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence)\n",
    "    #print('\\nIf there are more than 3 ms, change to 2\\n', sentence)\n",
    "    \n",
    "    # 불용어 제거\n",
    "    if remove_stopwords:\n",
    "        tkns = ' '.join(word for word in sentence.split() if not word in stop_words if len(word) > 1)\n",
    "        #print('\\nRemove Stopwords\\n', sentence)\n",
    "    # 불용어 미제거(Summary)\n",
    "    else:\n",
    "        tkns = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "        #print('\\nRemain Stopwords\\n', sentence)\n",
    "    return tkns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my option  is good\n"
     ]
    }
   ],
   "source": [
    "x = 'my option ( xyz  ) is good'\n",
    "x = re.sub(r'\\([^)]*\\)', '', x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 내부의 각 줄에 주석을 달았으므로 자세한 설명은 생략하겠습니다. 여기서는 Text 열에서는 불용어를 제거하고, Summary 열에서는 불용어를 제거하지 않기로 결정했습니다. Summary를 입력으로 할 때는 두번째 인자를 0으로 줘서 불용어를 제거하지 않는 버전을 실행하겠습니다. 임의의 Text 문장과 Summary 문장을 만들어 전처리 함수를 통한 전처리 후의 결과를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everything bought great infact ordered twice third ordered wasfor mother father\n",
      "great way to start the day\n"
     ]
    }
   ],
   "source": [
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Great way to start (or finish) the day!!!'\n",
    "print(preprocess_sentence(temp_text))\n",
    "print(preprocess_sentence(temp_summary,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 Text 열에 대해서 전처리를 수행하겠습니다. 전처리 후에는 5개의 전처리 된 샘플을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better',\n",
       " 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo',\n",
       " 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch',\n",
       " 'looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal',\n",
       " 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = []\n",
    "\n",
    "for s in df_data['Text']:\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "clean_text[:5]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Summary 열에 대해서 전처리를 수행하겠습니다. 전처리 후에는 5개의 전처리 된 샘플을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:282: UserWarning: \"http://www.amazon.com/gp/product/b007i7yygy/ref=cm_cr_rev_prod_title\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['good quality dog food',\n",
       " 'not as advertised',\n",
       " 'delight says it all',\n",
       " 'cough medicine',\n",
       " 'great taffy']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary\n",
    "clean_summary =[]\n",
    "for s in df_data['Summary']:\n",
    "    clean_summary.append(preprocess_sentence(s,0))\n",
    "clean_summary[:5]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['Text'] = clean_text\n",
    "df_data['Summary'] = clean_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "혹시 전처리 과정에서 빈 값이 생겼다면 Null 값으로 변경한 후에 Null 값을 가진 샘플이 생겼는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text        0\n",
      "Summary    70\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_data.replace('', np.nan, inplace=True)\n",
    "print(df_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary 열에서 70개의 샘플이 Null 값을 가집니다. 이 샘플들을 제거해주고, 전체 샘플수를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample count :  88355\n"
     ]
    }
   ],
   "source": [
    "df_data.dropna(axis=0,inplace = True)\n",
    "print('Total sample count : ', len(df_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Text 열과 Summary 열에 대해서 길이 분포를 확인해보겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length of Text  : 2\n",
      "Max length of Text  : 1235\n",
      "Average length of Text  : 38.792428272310566\n",
      "Min length of Summary  : 1\n",
      "Max length of Summary: 28\n",
      "Average length of Summary: 4.010729443721352\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 길이 분포 출력\n",
    "text_len  = [ len(s.split()) for s in df_data['Text']]\n",
    "summary_len = [len(s.split()) for s in df_data['Summary']]\n",
    "\n",
    "print('Min length of Text  : {}'.format(np.min(text_len)))\n",
    "print('Max length of Text  : {}'.format(np.max(text_len)))\n",
    "print('Average length of Text  : {}'.format(np.mean(text_len)))\n",
    "print('Min length of Summary  : {}'.format(np.min(summary_len)))\n",
    "print('Max length of Summary: {}'.format(np.max(summary_len)))\n",
    "print('Average length of Summary: {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Summary')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Summary')\n",
    "plt.hist(summary_len, bins=40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins=40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원문 텍스트는 대체적으로 100이하의 길이를 가집니다. 또한, 평균 길이는 38입니다. 요약의 경우에는 대체적으로 15이하의 길이를 가지며 평균 길이는 4입니다. 여기서 패딩의 길이를 정하겠습니다. 평균 길이보다는 크게 잡아 각각 50과 8로 결정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 50\n",
    "summary_max_len = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50과 8이라는 이 두 길이가 얼마나 많은 샘플들의 길이보다 큰지 확인해보겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        #print('\\ns:\\n',s, '\\ns.split\\n', s.split())\n",
    "        if (len(s.split()) <= max_len ):\n",
    "            cnt += 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s' %(max_len, (cnt/len(nested_list))))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 50 이하인 샘플의 비율: 0.7745119121724859\n"
     ]
    }
   ],
   "source": [
    "below_threshold_len(text_max_len, df_data['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text 열은 길이가 50 이하인 비율이 77%입니다. 약 23%의 샘플이 길이 50보다 큽니다. Summary 열에 대해서 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 8 이하인 샘플의 비율: 0.9424593967517402\n"
     ]
    }
   ],
   "source": [
    "below_threshold_len(summary_max_len, df_data['Summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary 열은 길이가 8 이하인 경우가 94%입니다. 여기서는 정해준 최대 길이보다 큰 샘플들은 제거하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples count :  65818\n"
     ]
    }
   ],
   "source": [
    "df_data = df_data[df_data['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "df_data = df_data[df_data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
    "\n",
    "print('Total samples count : ', len(df_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 샘플수가 65,818개로 줄었습니다. 정제 작업이 완료된 상위 샘플 5개를 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>not as advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says it all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary\n",
       "0  bought several vitality canned dog food produc...  good quality dog food\n",
       "1  product arrived labeled jumbo salted peanuts p...      not as advertised\n",
       "2  confection around centuries light pillowy citr...    delight says it all\n",
       "3  looking secret ingredient robitussin believe f...         cough medicine\n",
       "4  great taffy great price wide assortment yummy ...            great taffy"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq 훈련을 위해서는 디코더의 예측 대상에 시작 토큰과 종료 토큰을 추가할 필요가 있습니다. 시작 토큰은 sostoken, 종료 토큰은 eostoken이라 명명하고 앞, 뒤로 추가하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>sostoken good quality dog food eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>sostoken not as advertised eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>sostoken delight says it all eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>sostoken cough medicine eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>sostoken great taffy eostoken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  bought several vitality canned dog food produc...   \n",
       "1  product arrived labeled jumbo salted peanuts p...   \n",
       "2  confection around centuries light pillowy citr...   \n",
       "3  looking secret ingredient robitussin believe f...   \n",
       "4  great taffy great price wide assortment yummy ...   \n",
       "\n",
       "                                   Summary  \n",
       "0  sostoken good quality dog food eostoken  \n",
       "1      sostoken not as advertised eostoken  \n",
       "2    sostoken delight says it all eostoken  \n",
       "3         sostoken cough medicine eostoken  \n",
       "4            sostoken great taffy eostoken  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
    "df_data['Summary'] = df_data['Summary'].apply(lambda x : 'sostoken ' + x + ' eostoken')\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 각각 Text_data와 Summary_data로 저장하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = list(df_data['Text'])\n",
    "summary_data = list(df_data['Summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 데이터의 분리\n",
    "훈련 데이터와 테스트 데이터를 분리해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of train data :  52654\n",
      "Count of train label data :  52654\n",
      "Count of test data :  13164\n",
      "Count of test label data :  13164\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test, y_train, y_test = train_test_split(text_data, summary_data, test_size=0.2, random_state=0, shuffle=True)\n",
    "\n",
    "print('Count of train data : ', len(x_train))\n",
    "print('Count of train label data : ', len(y_train))\n",
    "print('Count of test data : ', len(x_test))\n",
    "print('Count of test label data : ', len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) 정수 인코딩\n",
    "이제 기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터에 정수 인코딩을 수행해야 합니다. 훈련 데이터에 대해서 단어 집합(vocaburary)을 만들어봅시다. 우선, 원문에 해당되는 X_train에 대해서 수행합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tkn = Tokenizer()\n",
    "src_tkn.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되었습니다. 이는 src_tokenizer.word_index에 저장되어져 있습니다. 여기서는 빈도수가 낮은 단어들은 자연어 처리에서 배제하고자 합니다. 등장 빈도수가 7회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 31940\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 23707\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 8233\n",
      "단어 집합에서 희귀 단어의 비율: 74.22354414527238\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.3949897589412323\n"
     ]
    }
   ],
   "source": [
    "threshold = 7\n",
    "total_cnt = len(src_tkn.word_index) # 단어의 수\n",
    "rare_cnt = 0  # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0  # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0  # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tkn.word_counts.items():\n",
    "    total_freq += value\n",
    "    \n",
    "    # 단어의 등장 빈도수가 threshold.보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt += 1\n",
    "        rare_freq += value\n",
    "        \n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "등장 빈도가 threshold 값인 7회 미만. 즉, 6회 이하인 단어들은 단어 집합에서 무려 70% 이상을 차지합니다. 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 적은 수치인 3.39%밖에 되지 않습니다. 여기서는 등장 빈도가 6회 이하인 단어들은 정수 인코딩 과정에서 배제시키고자 합니다. 위에서 이를 제외한 단어 집합의 크기를 8,233으로 계산했는데, 저자는 깔끔한 값을 선호하여 이와 비슷한 값으로 단어 집합의 크기를 8000으로 제한하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vc = 8000\n",
    "src_tknr = Tokenizer(num_words = src_vc)\n",
    "src_tknr.fit_on_texts(x_train)\n",
    "\n",
    "x_train = src_tknr.texts_to_sequences(x_train)\n",
    "x_test = src_tknr.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4256, 611, 816, 4477, 528, 1540, 32, 45, 4, 79, 2281, 234, 19, 1157, 2923, 948, 1122, 996, 2, 153, 10, 43], [204, 15, 154, 51, 259, 60, 73, 24, 49, 17, 886, 259, 2, 29, 209, 14, 504, 2081, 420, 13, 86, 64, 22, 2464, 134, 14, 504, 86], [112, 1570, 2, 1771, 185, 39, 505, 112, 283], [296, 105, 496, 834, 69, 103, 334, 5082, 2294, 765, 7, 908, 585, 1489, 351, 5265, 100, 75, 1908, 137, 141, 585, 864, 5083, 89, 830, 564, 1772, 305, 254, 1541, 97, 247, 2, 1600, 2100, 92, 2645], [54, 1247, 806, 5, 497, 479, 46, 236, 1218, 381, 5, 3879, 3880, 172, 479, 46, 1247]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_tknr = Tokenizer()\n",
    "tgt_tknr.fit_on_texts(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되었습니다. 이는 tar_tokenizer.word_index에 저장되어져 있습니다. 등장 빈도수가 6회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 10498\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 8126\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 2372\n",
      "단어 집합에서 희귀 단어의 비율: 77.40522004191274\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 4.833194065065198\n"
     ]
    }
   ],
   "source": [
    "threshold = 6\n",
    "total_cnt = len(tgt_tknr.word_index)  # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0  # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "for key, value in tgt_tknr.word_counts.items():\n",
    "    total_freq += value\n",
    "    \n",
    "    if (value < threshold):\n",
    "        rare_cnt += 1\n",
    "        rare_freq += value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "등장 빈도가 5회 이하인 단어들은 단어 집합에서 약 77%를 차지합니다. 하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 매우 적은 수치인 4.83%밖에 되지 않습니다. 이 단어들은 정수 인코딩 과정에서 배제시키겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_vc = 2000\n",
    "tgt_tknr = Tokenizer(num_words = tgt_vc)\n",
    "tgt_tknr.fit_on_texts(y_train)\n",
    "\n",
    "y_train = tgt_tknr.texts_to_sequences(y_train)\n",
    "y_test = tgt_tknr.texts_to_sequences(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 805, 2], [1, 7, 298, 138, 86, 2], [1, 25, 745, 2], [1, 488, 39, 22, 30, 12, 2], [1, 15, 16, 238, 84, 35, 12, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) 빈 샘플(empty samples) 제거\n",
    "전체 데이터에서 빈도수가 낮은 단어가 삭제되었다는 것은 빈도수가 낮은 단어만으로 구성되었던 샘플들은 이제 빈(empty) 샘플이 되었다는 것을 의미합니다. 이 현상은 길이가 상대적으로 길었던 원문(Text)의 경우에는 문제가 별로 없겠지만, 애초에 평균 길이가 4밖에 되지 않았던 요약문(Summary)의 경우에는 이 현상이 굉장히 두드러졌을 가능성이 높습니다. 요약문에서 길이가 0이 된 샘플들의 인덱스를 받아옵시다. 주의할 점은 요약문에는 sostoken과 eostoken이 추가된 상태이고, 이 두 토큰은 모든 샘플에서 등장하므로 빈도수가 샘플수와 동일하여 단어 집합 제한에도 삭제 되지 않습니다. 그래서 이제 길이가 0이 된 요약문의 실질적 길이는 2입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(y_train) if len(sentence) ==2]\n",
    "drop_test = [ index for index, sentence in  enumerate(y_test) if len(sentence) ==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터와 테스트 데이터에 대해서 요약문의 길이가 2인 경우의 인덱스를 각각 drop_train과 drop_test에 저장하였습니다. 삭제 전의 훈련 데이터와 테스트 데이터의 개수를 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 52654\n",
      "훈련 레이블의 개수 : 52654\n",
      "테스트 데이터의 개수 : 13164\n",
      "테스트 레이블의 개수 : 13164\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 개수 :', len(x_train))\n",
    "print('훈련 레이블의 개수 :',len(y_train))\n",
    "print('테스트 데이터의 개수 :',len(x_test))\n",
    "print('테스트 레이블의 개수 :',len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "삭제 후의 개수는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 51404\n",
      "훈련 레이블의 개수 : 51404\n",
      "테스트 데이터의 개수 : 12813\n",
      "테스트 레이블의 개수 : 12813\n"
     ]
    }
   ],
   "source": [
    "x_train = np.delete(x_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "x_test = np.delete(x_test, drop_test, axis=0)\n",
    "y_test = np.delete(y_test, drop_test, axis=0)\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(x_train))\n",
    "print('훈련 레이블의 개수 :',len(y_train))\n",
    "print('테스트 데이터의 개수 :',len(x_test))\n",
    "print('테스트 레이블의 개수 :',len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) 패딩하기\n",
    "훈련 데이터와 테스트 데이터에 대해서 패딩 작업을 수행합니다. 이미 앞서 정해둔 최대 길이를 넘는 샘플들은 제외했기 때문에 따로 길이 분포를 재확인하지는 않겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(x_train, maxlen = text_max_len, padding='post')\n",
    "x_test = pad_sequences(x_test, maxlen = text_max_len, padding='post')\n",
    "y_train = pad_sequences(y_train, maxlen = summary_max_len, padding='post')\n",
    "y_test = pad_sequences(y_test, maxlen = summary_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. seq2seq + attention으로 요약 모델 설계 및 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코더를 설계해보겠습니다. 인코더는 LSTM 층을 3개 쌓습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# encoder\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# Embedding Layer of encoder\n",
    "enc_emb = Embedding(src_vc, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# Encoder LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size,  return_sequences = True, return_state = True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# Encoder LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size,  return_sequences = True, return_state = True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# Encoder LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size,  return_sequences = True, return_state = True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더를 설계해보겠습니다. 단, 출력층은 제외하고 설계하겠습니다. 디코더의 설계는 인코더와 사실상 동일하지만 초기 상태(initial_state)를 인코더의 상태로 주어야 하는 것에 주의합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# Embedding Layer of decoder\n",
    "dec_emb = Embedding(tgt_vc, embedding_dim)(decoder_inputs)\n",
    "\n",
    "# LSTM of Decoder\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout =0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs,_,_ = decoder_lstm(dec_emb, initial_state = [ state_h, state_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 디코더의 출력층을 설계합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 50, 128)      1024000     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "unified_lstm_4 (UnifiedLSTM)    [(None, 50, 256), (N 394240      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "unified_lstm_5 (UnifiedLSTM)    [(None, 50, 256), (N 525312      unified_lstm_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 128)    256000      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "unified_lstm_6 (UnifiedLSTM)    [(None, 50, 256), (N 525312      unified_lstm_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "unified_lstm_7 (UnifiedLSTM)    [(None, None, 256),  394240      embedding_4[0][0]                \n",
      "                                                                 unified_lstm_6[0][1]             \n",
      "                                                                 unified_lstm_6[0][2]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2000)   514000      unified_lstm_7[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 3,633,104\n",
      "Trainable params: 3,633,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Output layer of decoder\n",
    "decoder_softmax_layer = Dense(tgt_vc, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "# Define Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.summary() 결과는 지면의 한계로 여기에 올리지 않겠습니다. 총 3,633,104개의 매개변수를 가진 seq2seq 모델이 설계됩니다. 지금까지의 모델 설계는 앞서 seq2seq 챕터에서 배웠던 내용과 동일합니다.\n",
    "\n",
    "그런데 이번 챕터에서는 어텐션 메커니즘을 사용할 예정이므로 위에서 설계한 출력층을 사용하지 않고, 어텐션 메커니즘이 결합된 새로운 출력층을 설계해보겠습니다. 어텐션 함수를 직접 작성하지 않고 이미 깃허브에 공개된 함수를 사용할 것이므로 아래의 코드를 통해 attention.py 파일을 다운로드하고, AttentionLayer를 임포트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/layers/attention.py\", filename=\"attention.py\")\n",
    "\n",
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어텐션 메커니즘을 이용해 디코더의 출력층을 새롭게 설계합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 50, 128)      1024000     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "unified_lstm_4 (UnifiedLSTM)    [(None, 50, 256), (N 394240      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "unified_lstm_5 (UnifiedLSTM)    [(None, 50, 256), (N 525312      unified_lstm_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 128)    256000      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "unified_lstm_6 (UnifiedLSTM)    [(None, 50, 256), (N 525312      unified_lstm_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "unified_lstm_7 (UnifiedLSTM)    [(None, None, 256),  394240      embedding_4[0][0]                \n",
      "                                                                 unified_lstm_6[0][1]             \n",
      "                                                                 unified_lstm_6[0][2]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2000)   514000      unified_lstm_7[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 3,633,104\n",
      "Trainable params: 3,633,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Attention Layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Attention결과와 Decoder hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis = -1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# Decoder output layer\n",
    "decoder_softmax_layer = Dense(tgt_vc, activation='softmax')\n",
    "deocder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# Define Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 4,276,432개의 파라미터를 가진 모델이 설계됩니다. 이제 모델을 컴파일합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조기 종료 조건을 설정하고 모델을 학습시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51404 samples, validate on 12813 samples\n",
      "Epoch 1/50\n",
      "51404/51404 [==============================] - 857s 17ms/sample - loss: 3.0592 - val_loss: 2.7562\n",
      "Epoch 2/50\n",
      "51404/51404 [==============================] - 843s 16ms/sample - loss: 2.7252 - val_loss: 2.6584\n",
      "Epoch 3/50\n",
      "51404/51404 [==============================] - 734s 14ms/sample - loss: 2.5894 - val_loss: 2.5165\n",
      "Epoch 4/50\n",
      "51404/51404 [==============================] - 809s 16ms/sample - loss: 2.4738 - val_loss: 2.4293\n",
      "Epoch 5/50\n",
      "51404/51404 [==============================] - 895s 17ms/sample - loss: 2.3896 - val_loss: 2.3637\n",
      "Epoch 6/50\n",
      "51404/51404 [==============================] - 820s 16ms/sample - loss: 2.3170 - val_loss: 2.3034\n",
      "Epoch 7/50\n",
      "51404/51404 [==============================] - 785s 15ms/sample - loss: 2.2558 - val_loss: 2.2628\n",
      "Epoch 8/50\n",
      "51404/51404 [==============================] - 893s 17ms/sample - loss: 2.2091 - val_loss: 2.2320\n",
      "Epoch 9/50\n",
      "51404/51404 [==============================] - 904s 18ms/sample - loss: 2.1698 - val_loss: 2.2134\n",
      "Epoch 10/50\n",
      "51404/51404 [==============================] - 964s 19ms/sample - loss: 2.1372 - val_loss: 2.1920\n",
      "Epoch 11/50\n",
      "51404/51404 [==============================] - 892s 17ms/sample - loss: 2.1079 - val_loss: 2.1813\n",
      "Epoch 12/50\n",
      "51404/51404 [==============================] - 816s 16ms/sample - loss: 2.0814 - val_loss: 2.1647\n",
      "Epoch 13/50\n",
      "51404/51404 [==============================] - 801s 16ms/sample - loss: 2.0577 - val_loss: 2.1506\n",
      "Epoch 14/50\n",
      "51404/51404 [==============================] - 765s 15ms/sample - loss: 2.0337 - val_loss: 2.1414\n",
      "Epoch 15/50\n",
      "51404/51404 [==============================] - 741s 14ms/sample - loss: 2.0125 - val_loss: 2.1325\n",
      "Epoch 16/50\n",
      "51404/51404 [==============================] - 743s 14ms/sample - loss: 1.9923 - val_loss: 2.1213\n",
      "Epoch 17/50\n",
      "51404/51404 [==============================] - 840s 16ms/sample - loss: 1.9727 - val_loss: 2.1161\n",
      "Epoch 18/50\n",
      "51404/51404 [==============================] - 833s 16ms/sample - loss: 1.9526 - val_loss: 2.1119\n",
      "Epoch 19/50\n",
      "51404/51404 [==============================] - 732s 14ms/sample - loss: 1.9346 - val_loss: 2.1046\n",
      "Epoch 20/50\n",
      "51404/51404 [==============================] - 732s 14ms/sample - loss: 1.9180 - val_loss: 2.1000\n",
      "Epoch 21/50\n",
      "51404/51404 [==============================] - 744s 14ms/sample - loss: 1.9015 - val_loss: 2.0932\n",
      "Epoch 22/50\n",
      "51404/51404 [==============================] - 740s 14ms/sample - loss: 1.8862 - val_loss: 2.0916\n",
      "Epoch 23/50\n",
      "51404/51404 [==============================] - 740s 14ms/sample - loss: 1.8707 - val_loss: 2.0907\n",
      "Epoch 24/50\n",
      "51404/51404 [==============================] - 906s 18ms/sample - loss: 1.8580 - val_loss: 2.0911\n",
      "Epoch 25/50\n",
      "51404/51404 [==============================] - 744s 14ms/sample - loss: 1.8442 - val_loss: 2.0898\n",
      "Epoch 26/50\n",
      "51404/51404 [==============================] - 744s 14ms/sample - loss: 1.8325 - val_loss: 2.0873\n",
      "Epoch 27/50\n",
      "51404/51404 [==============================] - 739s 14ms/sample - loss: 1.8183 - val_loss: 2.0866\n",
      "Epoch 28/50\n",
      "51404/51404 [==============================] - 740s 14ms/sample - loss: 1.8074 - val_loss: 2.0855\n",
      "Epoch 29/50\n",
      "51404/51404 [==============================] - 747s 15ms/sample - loss: 1.7962 - val_loss: 2.0835\n",
      "Epoch 30/50\n",
      "51404/51404 [==============================] - 732s 14ms/sample - loss: 1.7854 - val_loss: 2.0849\n",
      "Epoch 31/50\n",
      "42752/51404 [=======================>......] - ETA: 1:56 - loss: 1.7735"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "history = model.fit([x_train, y_train[:,:-1]] \\\n",
    "                    , y_train.reshape(y_train.shape[0], y_train.shape[1],1)[:,1:] \\\n",
    "                    , epochs =50, callbacks=[es], batch_size=256 \\\n",
    "                    , validation_data =([x_test, y_test[:,:-1]] \\\n",
    "                    , y_test.reshape(y_test.shape[0],y_test.shape[1],1)[:,1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 과정하면서 기록된 훈련 데이터의 손실과 테스트 데이터의 손실 히스토리를 시각화하여 출력합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 데이터의 손실이 지속적으로 줄어들다가 어느 순간부터 정체하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. seq2seq + attention으로 요약 모델 테스트하기\n",
    "테스트를 위해 필요한 3개의 사전을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tknr.index_word\n",
    "tgt_word_to_index = tgt_tknr.word_index\n",
    "tgt_index_to_word = tgt_tknr.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq는 훈련 단계와 테스트 단계의 동작이 다르므로 테스트 단계의 모델을 별도로 다시 설계해줄 필요가 있습니다. 다시 새로운 seq2seq 모델을 만들겠습니다. 우선 인코더를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design encoder\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 테스트 단계의 디코더를 설계합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 시스템의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = \\\n",
    "    decoder_lstm(des_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attn_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-c3389c668869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Attention function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdecoder_hidden_state_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_max_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mattn_out_inf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_states_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_hidden_state_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_outputs2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Decoder's output layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attn_layer' is not defined"
     ]
    }
   ],
   "source": [
    "# Attention function\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "\n",
    "# Decoder's output layer\n",
    "decoder_outputs2= decoder_softmax_layer(decoder_inf_concat)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "            [decoder_inputs] \n",
    "            + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c]\n",
    "            , [decoder_outputs2] + [state_h2, state_c2]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 단계를 위한 모델이 완성되었습니다. 테스트를 위해 사용되는 함수 decode_sequence를 설계합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로 부터 인코더의 상태를 얻음\n",
    "    states_values = encoder_model.predict(input_seq)\n",
    "    # SOS에 해당하는 원핫 백터 생성\n",
    "    tgt_seq = np.zeros((1,1,tgt_vc_size))\n",
    "    tgt_seq[0,0, tgt_to_index['\\t']] =1.\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence =''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        # 이전 시점의 상태 sates_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([tgt_seq]+sates_value)\n",
    "        sampled_tkn_index = np.argmax(output_tkns[0,-1,:])\n",
    "        sampled_char = index_to_tgt[sampled_tkn_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        # eos에 도달하거나 최대 길이를 넘으면 중단\n",
    "        if (sampled_char =='\\n' or\n",
    "           len(decoded_sentence) > max_tgt_len):\n",
    "            stop_condition= True\n",
    "            \n",
    "        # 길이가 1인 타켓 시퀀스를 업데이트\n",
    "        tgt_seq = np.zeros((1,1,tgt_vc_size))\n",
    "        tgt_seq[0,0,sampled_tkn_index] =1.\n",
    "        \n",
    "        # 상태를 업데이트\n",
    "        states_value =[ h,c]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 단계에서 원문과 실제 요약문, 예측 요약문을 비교하기 위해 정수 시퀀스를 텍스트 시퀀스로 만드는 함수를 설계합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp = ''\n",
    "    for i in input_seq:\n",
    "        if(i !=0):\n",
    "            temp += src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp = ''\n",
    "    for i in input_seq:\n",
    "        if((i != 0 and i != tgt_word_index['sostoken']) \n",
    "           and i != tgt_word_index['eostoken']):\n",
    "            temp += tgt_index_to_word[i] + ' '\n",
    "    return temp\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 샘플 중 500번부터 1000번까지 테스트해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reange(500, 1000):\n",
    "    print('원문 : ', seq2text(x_test[i]))\n",
    "    print('실제 요약문 : ', seq2summary(x_test[i]))\n",
    "    print('예측요약문 : ', decode_sequence(x_test[i].reshape(1, text_max_len)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 요약문과 완전히 똑같지 않으면서 원문의 맥락을 잘 잡아서 예측된 요약문들이 존재하는 것을 확인할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
